{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tr0HeVXDwOPo"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import datetime\n",
    "import json\n",
    "i = 1\n",
    "# Get the current year\n",
    "year = datetime.datetime.now().year\n",
    "\n",
    "sous_sous_formations_uniques = set()\n",
    "url_formations = 'https://www.cegos.fr/formations'\n",
    "headers = {\n",
    "    'User-Agent': 'Opera_gx'\n",
    "}\n",
    "\n",
    "response = requests.get(url_formations, headers=headers)\n",
    "\n",
    "if response.ok:\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "    formations_dict = {}  # Dictionnaire pour stocker les formations et leurs informations\n",
    "\n",
    "    lis = soup.find_all('li', class_='city__list-item')\n",
    "    for li in lis:\n",
    "        les_a = li.find_all('a')\n",
    "        for a in les_a:\n",
    "            if 'href' in a.attrs:\n",
    "                href = a['href']\n",
    "                if href.startswith('/formations/'):\n",
    "\n",
    "                    full_link = f'https://www.cegos.fr{href}'  # Construire le lien complet\n",
    "                    formation = a.text.strip()  # Nettoyer la chaîne en supprimant les espaces superflus\n",
    "\n",
    "                    # Créez un dictionnaire pour chaque formation avec des informations supplémentaires\n",
    "                    formation_info = {\n",
    "                        'Lien': full_link,\n",
    "                    }\n",
    "\n",
    "                    if formation not in formations_dict:\n",
    "                        formations_dict[formation] = {}\n",
    "\n",
    "                    formations_dict[formation]['info'] = formation_info  # Associez les informations de la formation\n",
    "\n",
    "    # Parcourez le dictionnaire des formations\n",
    "    for formation, info in formations_dict.items():\n",
    "        url_sous_formations = info['info']['Lien']\n",
    "        headers = {\n",
    "            'User-Agent': 'Opera_gx'\n",
    "        }\n",
    "\n",
    "        response = requests.get(url_sous_formations, headers=headers)\n",
    "\n",
    "        if response.ok:\n",
    "            soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "            # Supposons que vous voulez filtrer les éléments par attribut data-year\n",
    "            year = str(year)  # Remplacez par la valeur que vous recherchez\n",
    "\n",
    "            lus = soup.find_all('ul', class_='domaine__formations-items')  # Utilisez 'ul' au lieu de 'lu'\n",
    "            for lu in lus:\n",
    "                lis = lu.find_all('li', {'data-year': year})\n",
    "                for li in lis:\n",
    "                    h3 = li.find('h3', class_='u-myl')\n",
    "                    sous_formation = h3.find('strong').text\n",
    "\n",
    "#                    print(sous_formation)\n",
    "\n",
    "                    a_sous_sous_formation = li.find_all('a', class_='domaine__formation-link')\n",
    "\n",
    "                    sous_formations_dict = {}\n",
    "                    #Les noms de sous_sous_formations incluent le nombre de jours et d'autres informations donc on extrait que le nom.\n",
    "#=========================================================\n",
    "\n",
    "                    for a in a_sous_sous_formation:\n",
    "\n",
    "                        sous_sous_formation1 = a.find('div', class_='domaine__formation-title')\n",
    "                        sous_sous_formation = sous_sous_formation1.text.strip()\n",
    "#=========================================================\n",
    "                        # Initialisation de l'ensemble pour stocker les identifiants uniques des sous-sous-formations\n",
    "                        if sous_sous_formation not in sous_sous_formations_uniques:\n",
    "                          sous_sous_formation_info = {\n",
    "                              'Lien': f'https://www.cegos.fr{a[\"href\"]}',\n",
    "                              'Prix': 0,\n",
    "                              'Duree' : 0,\n",
    "                              'Description' : '',\n",
    "                              'note' : '',\n",
    "                              'nombre_avis' : '',\n",
    "                              'niveau' : '',\n",
    "                              'Pour Qui': ''\n",
    "                          }\n",
    "                          sous_formations_dict[sous_sous_formation] = sous_sous_formation_info\n",
    "                          sous_sous_formations_uniques.add(sous_sous_formation)\n",
    "                    info[sous_formation] = sous_formations_dict\n",
    "# Convertir le dictionnaire en JSON\n",
    "json_data = json.dumps(formations_dict, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Nom du fichier de sortie\n",
    "output_filename = 'formations.json'\n",
    "\n",
    "# Écrire le JSON dans le fichier\n",
    "with open(output_filename, 'w', encoding='utf-8') as json_file:\n",
    "    json_file.write(json_data)\n",
    "\n",
    "print(f'Données JSON écrites dans {output_filename}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-76ww9ALFybR"
   },
   "outputs": [],
   "source": [
    "formations_dict['Droit des affaires']['Formation juridique - Droit des sociétés']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "gdS0hrX-qMBE"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "\n",
    "def extraire_informations_formation(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Opera_gx'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "        note_text = None  # Initialisation de la variable note_text à None\n",
    "        avis_text = None\n",
    "                # Initialisation des variables à None\n",
    "        note_text = None\n",
    "        avis_text = None\n",
    "        price = None\n",
    "        duration_str = None\n",
    "        description = None\n",
    "        level = None\n",
    "        Pour_Qui = None\n",
    "\n",
    "        # Recherche de tous les éléments <a> avec la classe \"product__rating\"\n",
    "        rating_links = soup.find_all('a', class_='product__rating')\n",
    "\n",
    "        for rating_link in rating_links:\n",
    "            # Vérifie si l'élément <a> contient la classe \"product__rating\"\n",
    "            if 'product__rating' in rating_link['class']:\n",
    "                # Recherche de l'élément avec la classe 'rating__note' à l'intérieur de cet élément\n",
    "                rating_note = rating_link.find('div', class_='rating__note')\n",
    "                if rating_note:\n",
    "                    note_text = rating_note.get_text()\n",
    "\n",
    "                    # Recherche de l'élément avec la classe 'rating__count' pour obtenir le nombre d'avis\n",
    "                    rating_count = rating_link.find('div', class_='rating__count')\n",
    "                    if rating_count:\n",
    "                        avis_text = rating_count.get_text().strip()\n",
    "                    else:\n",
    "                        avis_text = None\n",
    "\n",
    "        script = soup.find(\"script\", string=re.compile(\"let mainProduct = {.*};\", re.DOTALL))\n",
    "\n",
    "        if script:\n",
    "            match_price = re.search(r'\"price_without_vat\":\"(.*?)\"', script.string)\n",
    "            match_duration_days = re.search(r'\"duration_classroom_days\":\\s*\"*(\\d+\\.?\\d*)', script.string, re.I)\n",
    "            match_duration_min = re.search(r'\"duration_classroom_min\":(\\d+)', script.string)\n",
    "\n",
    "            price = match_price.group(1) if match_price else None\n",
    "\n",
    "            if match_duration_days and match_duration_min:\n",
    "                # Convertir la durée en nombre à virgule flottante\n",
    "                duration_days = float(match_duration_days.group(1))\n",
    "                # Convertir la durée en un entier\n",
    "                duration_days = int(duration_days)\n",
    "\n",
    "                duration_min = int(match_duration_min.group(1))\n",
    "                # Convertir la durée en heures et minutes\n",
    "                hours = duration_min // 60\n",
    "                minutes = duration_min % 60\n",
    "\n",
    "                # Créer la chaîne de durée au format 'x jours (y heures)'\n",
    "                duration_str = f'{duration_days} jours ({hours} heures)'\n",
    "            else:\n",
    "                duration_str = None\n",
    "\n",
    "        # Recherchez la balise <meta> avec l'attribut name=\"Description\"\n",
    "        meta_description = soup.find(\"meta\", attrs={\"name\": \"Description\"})\n",
    "\n",
    "        # Obtenez le contenu de l'attribut \"content\"\n",
    "        description = meta_description.get(\"content\") if meta_description else None\n",
    "\n",
    "        #----------------------------\n",
    "\n",
    "        level_search = soup.find('div',class_ = \"product__for product__for--required\")\n",
    "\n",
    "        # Vérifier si le contenu de la liste contient le mot 'Aucun'\n",
    "        if '\"Aucun.\"' in level_search.get_text():\n",
    "            level = \"débutant\"\n",
    "        else:\n",
    "          level = \"non débutant\"\n",
    "\n",
    "\n",
    "        Pour_Qui_search = soup.find('div',class_ = \"product__for product__for--whom\")\n",
    "\n",
    "        # Extraire tous les éléments li dans une liste Python\n",
    "        Pour_Qui = [li.get_text().replace('::marker', '').replace('\"', '').replace('.', '').replace('\\xa0', ' ').strip() for li in Pour_Qui_search.find_all('li')]\n",
    "        # Pour remplacer les espaces insécables par des espaces standards\n",
    "\n",
    "        return note_text, avis_text, price, duration_str, description, level, Pour_Qui\n",
    "\n",
    "    return None, None, None, None, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VmbIgka4opk9",
    "outputId": "74422aab-1a04-4f7f-af60-c76298a926bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les 20 formations ont été mis à jour dans formations.json\n"
     ]
    }
   ],
   "source": [
    "input_filename = 'formations.json'\n",
    "\n",
    "with open(input_filename, 'r', encoding='utf-8') as json_file:\n",
    "    formations_dict = json.load(json_file)\n",
    "\n",
    "# Parcourir et mettre à jour les sous-champs \"Prix\" pour les 5 premières formations\n",
    "count = 0\n",
    "for formation, formation_info in formations_dict.items():\n",
    "    for sous_formation, sous_formation_info in formation_info.items():\n",
    "        if count < 200 and isinstance(sous_formation_info, dict):\n",
    "            for sous_sous_formation, sous_sous_formation_info in sous_formation_info.items():\n",
    "                if count < 200 and isinstance(sous_sous_formation_info, dict):\n",
    "                    link_sous_sous_formation = sous_sous_formation_info['Lien']\n",
    "                    note, avis, price, duration, description, level, Pour_Qui = extraire_informations_formation(link_sous_sous_formation)\n",
    "\n",
    "                    sous_sous_formation_info['Duree'] = duration\n",
    "                    sous_sous_formation_info['Prix'] = price\n",
    "                    sous_sous_formation_info['Description'] = description\n",
    "                    sous_sous_formation_info['note'] = note\n",
    "                    sous_sous_formation_info['nombre_avis'] = avis\n",
    "                    sous_sous_formation_info['niveau'] = level\n",
    "                    sous_sous_formation_info['Pour Qui'] = Pour_Qui\n",
    "                    count += 1\n",
    "\n",
    "# Une fois que vous avez effectué toutes les modifications nécessaires, vous pouvez sauvegarder les données mises à jour dans le fichier JSON\n",
    "with open(input_filename, 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(formations_dict, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f'Les 20 formations ont été mis à jour dans {input_filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vgEifwxAwmvA",
    "outputId": "aefbc4b6-1553-4321-e944-28c1497894ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unidecode\n",
      "  Downloading Unidecode-1.3.7-py3-none-any.whl (235 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: unidecode\n",
      "Successfully installed unidecode-1.3.7\n"
     ]
    }
   ],
   "source": [
    "pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "gnGmh7dkyCum",
    "outputId": "27361ba4-ad3d-4090-dc35-15dd227c31ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "commercial, ventes\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import unidecode\n",
    "import re\n",
    "\n",
    "# Télécharger les stop words (à faire une fois)\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "def clean_course_name(course_name):\n",
    "    # Votre liste initiale de mots à exclure\n",
    "    custom_exclude_words = {\n",
    "        'et', 'des', 'les', 'de', 'la', 'le', 'en', 'du', 'pour', 'comment',\n",
    "        'avec', 'sans', 'sous', 'par', 'dans', 'sur', 'entre', 'contre', 'vers',\n",
    "        'après', 'avant', 'chez', 'pendant', 'depuis', 'jusque', 'jusqu', 'tandis',\n",
    "        'que', 'comme', 'si', 'lorsque', 'puisque', 'quoique', 'bien', 'ainsi'\n",
    "    }\n",
    "\n",
    "    # Stop words en français et en anglais\n",
    "    stop_words_fr = set(stopwords.words('french'))\n",
    "    stop_words_en = set(stopwords.words('english'))\n",
    "    exclude_words = custom_exclude_words.union(stop_words_fr, stop_words_en)\n",
    "\n",
    "    # Remplacer les caractères accentués\n",
    "    course_name = unidecode.unidecode(course_name)\n",
    "\n",
    "    # Supprimer le caractère avant les guillemets doubles\n",
    "    course_name = re.sub(r\".''\", '', course_name)\n",
    "\n",
    "    # Convertir en minuscules, supprimer les caractères spéciaux (sauf les tirets, les barres obliques)\n",
    "    course_name = re.sub(r'[^A-Za-z\\s/-]', '', course_name.lower())\n",
    "\n",
    "    # Split sur les espaces, les tirets, et les barres obliques\n",
    "    words = re.split(r'[\\s/-]+', course_name)\n",
    "\n",
    "    # Filtrer les mots, les mettre au singulier si nécessaire et exclure les mots d'une lettre\n",
    "    filtered_words = [word for word in words if word not in exclude_words and len(word) > 1]\n",
    "\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "\n",
    "# Exemple d'utilisation\n",
    "course_name = \"Commercial - Ventes\"\n",
    "cleaned_name = clean_course_name(course_name)\n",
    "print(cleaned_name)  # Devrait afficher une liste de mots clés nettoyés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "id": "5hzIpLRtxphU",
    "outputId": "362d23cb-fcc8-43ae-ca59-3302b68672c5"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-21-0c3514199acd>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    python -m spacy download fr_core_news_sm\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "python -m spacy download fr_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "6RIZlDuAxbtS",
    "outputId": "a7cd45f9-2490-4403-ebeb-dc33589cf3da"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS as fr_stop\n",
    "\n",
    "# Charger le modèle français\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "def filter_keywords_spacy(sentence):\n",
    "    # Tokenize la phrase avec Spacy\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "\n",
    "    # Filtrer les stop words et les mots non significatifs, sauf pour les exceptions\n",
    "    keywords = [token.text for token in doc if (token.text.lower() not in fr_stop and token.pos_ in [\"ADJ\",\"NOUN\", \"VERB\", \"PROPN\"])]\n",
    "\n",
    "    return ', '.join(keywords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fH3nZqk6NhMz"
   },
   "source": [
    "INSERT SCRIPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "2yMeJBBE3tUX"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Noms des fichiers\n",
    "input_json_filename = \"C:/Users/aanis/Desktop/P03/Données/CEGOS/formations_CEGOS.json\"  # Nom de votre fichier JSON\n",
    "output_sql_formation_filename = 'insert_formation.sql'\n",
    "output_sql_sous_formation_filename = 'insert_sous_formation.sql'\n",
    "output_sql_sous_sous_formation_filename = 'insert_sous_sous_formation.sql'\n",
    "output_csv_formation_filename = 'insert_formation.csv'\n",
    "output_csv_sous_formation_filename = 'insert_sous_formation.csv'\n",
    "output_csv_sous_sous_formation_filename = 'insert_sous_sous_formation.csv'\n",
    "\n",
    "# Ouvrir le fichier JSON pour lecture\n",
    "with open(input_json_filename, 'r', encoding='utf-8') as json_file:\n",
    "    formations_dict = json.load(json_file)\n",
    "\n",
    "# Ouvrir les fichiers SQL et CSV pour écriture\n",
    "with open(output_sql_formation_filename, 'w', encoding='utf-8') as sql_formation_file, \\\n",
    "     open(output_sql_sous_formation_filename, 'w', encoding='utf-8') as sql_sous_formation_file, \\\n",
    "     open(output_sql_sous_sous_formation_filename, 'w', encoding='utf-8') as sql_sous_sous_formation_file, \\\n",
    "     open(output_csv_formation_filename, 'w', newline='', encoding='utf-8') as csv_formation_file, \\\n",
    "     open(output_csv_sous_formation_filename, 'w', newline='', encoding='utf-8') as csv_sous_formation_file, \\\n",
    "     open(output_csv_sous_sous_formation_filename, 'w', newline='', encoding='utf-8') as csv_sous_sous_formation_file:\n",
    "\n",
    "\n",
    "    csv_formation_writer = csv.writer(csv_formation_file)\n",
    "    csv_sous_formation_writer = csv.writer(csv_sous_formation_file)\n",
    "    csv_sous_sous_formation_writer = csv.writer(csv_sous_sous_formation_file)\n",
    "\n",
    "    # Écrire les en-têtes pour chaque fichier CSV\n",
    "    csv_formation_writer.writerow(['RefDomaine', 'DomaineDeFormation','MotsCles'])\n",
    "    csv_sous_formation_writer.writerow(['RefSousDomaine', 'RefDomaine', 'SousDomaineDeFormation','MotsCles'])\n",
    "    csv_sous_sous_formation_writer.writerow(['RefSOUSSOUSDomaineF', 'RefSOUSDomaineF', 'Le_nom', 'Description', 'Notes', 'Nombre_avis', 'Duree', 'Niveau', 'Liens','MotsCles'])\n",
    "\n",
    "    numéro_formation = 1\n",
    "\n",
    "\n",
    "    for formation, formation_info in formations_dict.items():\n",
    "        numéro_sous_formation = 1\n",
    "        # Ref and corrected formation name\n",
    "        ref_domaine = f\"CEGOS_DomaineF-{numéro_formation:02}\"\n",
    "        formation_corrigée = formation.replace(\"'\", \"''\")\n",
    "\n",
    "        mots_clefs_formation1 = clean_course_name(formation_corrigée)\n",
    "        mots_clefs_formation = filter_keywords_spacy(mots_clefs_formation1)\n",
    "\n",
    "        # SQL Insert\n",
    "        sql_formation_insert = f\"INSERT INTO DomaineFormation (RefDomaine, DomaineDeFormation,MotsCles) VALUES ('{ref_domaine}', '{formation_corrigée}','{mots_clefs_formation}');\\n\"\n",
    "        sql_formation_file.write(sql_formation_insert)\n",
    "\n",
    "        # CSV Insert\n",
    "        csv_formation_writer.writerow([ref_domaine, formation_corrigée,mots_clefs_formation])\n",
    "\n",
    "        for sous_formation, sous_formation_info in formation_info.items():\n",
    "\n",
    "            numéro_sous_sous_formation = 1\n",
    "            if sous_formation == 'info':\n",
    "                continue\n",
    "\n",
    "            # Ref and corrected sub-formation name\n",
    "            ref_sous_domaine = ref_domaine + f\"-{numéro_sous_formation:02}\"\n",
    "            sous_formation_corrigée = sous_formation.replace(\"'\", \"''\")\n",
    "\n",
    "            mots_clefs_sous_formation1 = clean_course_name(sous_formation_corrigée)\n",
    "            mots_clefs_sous_formation = filter_keywords_spacy(mots_clefs_sous_formation1)\n",
    "            # SQL Insert\n",
    "            sql_sous_formation_insert = f\"INSERT INTO SousDomaineFormation (RefSousDomaine, RefDomaine, SousDomaineDeFormation, MotsCles) VALUES ('{ref_sous_domaine}', '{ref_domaine}', '{sous_formation_corrigée}','{mots_clefs_sous_formation}');\\n\"\n",
    "            sql_sous_formation_file.write(sql_sous_formation_insert)\n",
    "\n",
    "            # CSV Insert\n",
    "            csv_sous_formation_writer.writerow([ref_sous_domaine, ref_domaine, sous_formation_corrigée,mots_clefs_sous_formation])\n",
    "\n",
    "            numéro_sous_formation += 1\n",
    "\n",
    "            for sous_sous_formation, sous_sous_formation_info in sous_formation_info.items():\n",
    "                ref_sous_sous_domaine = ref_sous_domaine + f\"-{numéro_sous_sous_formation:02}\"\n",
    "                sous_sous_formation_corrigé = sous_sous_formation.replace(\"'\", \"''\")\n",
    "\n",
    "                mots_clefs_sous_sous_formation1 = clean_course_name(sous_sous_formation_corrigé)\n",
    "                mots_clefs_sous_sous_formation = filter_keywords_spacy(mots_clefs_sous_sous_formation1)\n",
    "\n",
    "                # Replace apostrophes in each string value\n",
    "                for key in sous_sous_formation_info:\n",
    "                    if isinstance(sous_sous_formation_info[key], str):\n",
    "                        sous_sous_formation_info[key] = sous_sous_formation_info[key].replace(\"'\", \"''\")\n",
    "\n",
    "                # SQL Insert\n",
    "                sql_sous_sous_formation_insert = (\n",
    "                    \"INSERT INTO SOUSSOUSDomaineFormation \"\n",
    "                    \"(RefSOUSSOUSDomaineF, RefSOUSDomaineF, Le_nom, Description, Notes, Nombre_avis, Duree, Niveau, Liens, MotsCles) VALUES \"\n",
    "                    f\"('{ref_sous_sous_domaine}', '{ref_sous_domaine}', '{sous_sous_formation_corrigé}', '{sous_sous_formation_info['Description']}', '{sous_sous_formation_info['note']}', '{sous_sous_formation_info['nombre_avis']}', '{sous_sous_formation_info['Duree']}', '{sous_sous_formation_info['niveau']}', '{sous_sous_formation_info['Lien']}','{mots_clefs_sous_sous_formation}');\\n\"\n",
    "                )\n",
    "                sql_sous_sous_formation_file.write(sql_sous_sous_formation_insert)\n",
    "\n",
    "                # CSV Insert\n",
    "                csv_sous_sous_formation_writer.writerow([ref_sous_sous_domaine, ref_sous_domaine, sous_sous_formation_corrigé, sous_sous_formation_info['Description'], sous_sous_formation_info['note'], sous_sous_formation_info['nombre_avis'], sous_sous_formation_info['Duree'], sous_sous_formation_info['niveau'], sous_sous_formation_info['Lien'],mots_clefs_sous_sous_formation])\n",
    "\n",
    "                numéro_sous_sous_formation += 1\n",
    "\n",
    "        numéro_formation += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fpFeyK0tlDa7",
    "outputId": "b9e0f412-f079-4f7b-8ed0-5f2c47161370"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "\n",
    "# Télécharger les stop words (à faire une fois)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def clean_course_name_coursera(course_name):\n",
    "    # Votre liste initiale de mots à exclure\n",
    "    custom_exclude_words = {\n",
    "        'et', 'des', 'les', 'de', 'la', 'le', 'en', 'du', 'pour', 'comment',\n",
    "        'avec', 'sans', 'sous', 'par', 'dans', 'sur', 'entre', 'contre', 'vers',\n",
    "        'après', 'avant', 'chez', 'pendant', 'depuis', 'jusque', 'jusqu', 'tandis',\n",
    "        'que', 'comme', 'si', 'lorsque', 'puisque', 'quoique', 'bien', 'ainsi'\n",
    "    }\n",
    "\n",
    "    # Stop words en français et en anglais\n",
    "    stop_words_fr = set(stopwords.words('french'))\n",
    "    stop_words_en = set(stopwords.words('english'))\n",
    "    exclude_words = custom_exclude_words.union(stop_words_fr, stop_words_en)\n",
    "\n",
    "    # Remplacer les caractères accentués\n",
    "    course_name = unidecode(course_name)\n",
    "\n",
    "\n",
    "    # Supprimer le caractère avant les guillemets doubles\n",
    "    course_name = re.sub(r\".''\", '', course_name)\n",
    "\n",
    "    # Convertir en minuscules, supprimer les caractères spéciaux (sauf les tirets, les barres obliques)\n",
    "    course_name = re.sub(r'[^A-Za-z\\s/-]', '', course_name.lower())\n",
    "\n",
    "    # Split sur les espaces, les tirets, et les barres obliques\n",
    "    words = re.split(r'[\\s/-]+', course_name)\n",
    "\n",
    "    # Filtrer les mots, les mettre au singulier si nécessaire et exclure les mots d'une lettre\n",
    "    filtered_words = [word for word in words if word not in exclude_words and len(word) > 1]\n",
    "\n",
    "    return \", \".join(filtered_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "K6fAz9ftktSe"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Coursera.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m output_csv_sous_sous_formation_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCoursera_insert_sous_sous_formation.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Read JSON file\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minput_json_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m json_file:\n\u001b[0;32m     18\u001b[0m     formations_dict \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(json_file)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Open SQL and CSV files for writing\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    308\u001b[0m     )\n\u001b[1;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Coursera.json'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "\n",
    "# Define your JSON file name\n",
    "input_json_filename = 'Coursera.json'\n",
    "\n",
    "# Output file names\n",
    "output_sql_formation_filename = 'Coursera_insert_formation.sql'\n",
    "output_sql_sous_formation_filename = 'Coursera_insert_sous_formation.sql'\n",
    "output_sql_sous_sous_formation_filename = 'Coursera_nsert_sous_sous_formation.sql'\n",
    "output_csv_formation_filename = 'Coursera_insert_formation.csv'\n",
    "output_csv_sous_formation_filename = 'Coursera_insert_sous_formation.csv'\n",
    "output_csv_sous_sous_formation_filename = 'Coursera_insert_sous_sous_formation.csv'\n",
    "\n",
    "# Read JSON file\n",
    "with open(input_json_filename, 'r', encoding='utf-8') as json_file:\n",
    "    formations_dict = json.load(json_file)\n",
    "\n",
    "# Open SQL and CSV files for writing\n",
    "with open(output_sql_formation_filename, 'w', encoding='utf-8') as sql_formation_file, \\\n",
    "     open(output_sql_sous_formation_filename, 'w', encoding='utf-8') as sql_sous_formation_file, \\\n",
    "     open(output_sql_sous_sous_formation_filename, 'w', encoding='utf-8') as sql_sous_sous_formation_file, \\\n",
    "     open(output_csv_formation_filename, 'w', newline='', encoding='utf-8') as csv_formation_file, \\\n",
    "     open(output_csv_sous_formation_filename, 'w', newline='', encoding='utf-8') as csv_sous_formation_file, \\\n",
    "     open(output_csv_sous_sous_formation_filename, 'w', newline='', encoding='utf-8') as csv_sous_sous_formation_file:\n",
    "\n",
    "    # CSV Writers\n",
    "    csv_formation_writer = csv.writer(csv_formation_file)\n",
    "    csv_sous_formation_writer = csv.writer(csv_sous_formation_file)\n",
    "    csv_sous_sous_formation_writer = csv.writer(csv_sous_sous_formation_file)\n",
    "\n",
    "    # Write headers for each CSV file\n",
    "    csv_formation_writer.writerow(['RefDomaine', 'DomaineDeFormation', 'MotsCles'])\n",
    "    csv_sous_formation_writer.writerow(['RefSousDomaine', 'RefDomaine', 'SousDomaineDeFormation', 'MotsCles'])\n",
    "    csv_sous_sous_formation_writer.writerow(['RefSOUSSOUSDomaineF', 'RefSOUSDomaineF', 'Le_nom', 'Description', 'Notes', 'Nombre_avis', 'Duree', 'Niveau', 'Liens', 'MotsCles'])\n",
    "\n",
    "    numero_formation = 1\n",
    "\n",
    "    for formation, sous_domaines in formations_dict.items():\n",
    "        ref_domaine = f\"COURSERA_DomaineF-{numero_formation:02}\"\n",
    "        formation_corrigee = formation.replace(\"'\", \"''\")\n",
    "        mots_clefs_formation = clean_course_name_coursera(formation_corrigee)\n",
    "\n",
    "        # SQL and CSV for formation\n",
    "        sql_formation_insert = f\"INSERT INTO DomaineFormation (RefDomaine, DomaineDeFormation, MotsCles) VALUES ('{ref_domaine}', '{formation_corrigee}', '{mots_clefs_formation}');\\n\"\n",
    "        sql_formation_file.write(sql_formation_insert)\n",
    "        csv_formation_writer.writerow([ref_domaine, formation_corrigee, mots_clefs_formation])\n",
    "\n",
    "        numero_sous_formation = 1\n",
    "        for sous_formation, cours in sous_domaines.items():\n",
    "            ref_sous_domaine = f\"{ref_domaine}-{numero_sous_formation:02}\"\n",
    "            sous_formation_corrigee = sous_formation.replace(\"'\", \"''\")\n",
    "            mots_clefs_sous_formation = clean_course_name_coursera(sous_formation_corrigee)\n",
    "\n",
    "            # SQL and CSV for sous_formation\n",
    "            sql_sous_formation_insert = f\"INSERT INTO SousDomaineFormation (RefSousDomaine, RefDomaine, SousDomaineDeFormation, MotsCles) VALUES ('{ref_sous_domaine}', '{ref_domaine}', '{sous_formation_corrigee}', '{mots_clefs_sous_formation}');\\n\"\n",
    "            sql_sous_formation_file.write(sql_sous_formation_insert)\n",
    "            csv_sous_formation_writer.writerow([ref_sous_domaine, ref_domaine, sous_formation_corrigee, mots_clefs_sous_formation])\n",
    "\n",
    "            numero_sous_sous_formation = 1\n",
    "            for cours_nom, cours_info in cours.items():\n",
    "                ref_sous_sous_domaine = f\"{ref_sous_domaine}-{numero_sous_sous_formation:02}\"\n",
    "                cours_nom_corrigé = cours_nom.replace(\"'\", \"''\")\n",
    "                mots_clefs_cours = clean_course_name_coursera(cours_nom_corrigé)\n",
    "\n",
    "                # Prepare data for SQL and CSV\n",
    "                description = cours_info.get('Course_description', '').replace(\"'\", \"''\")\n",
    "                notes = cours_info.get('stars', '')\n",
    "                nombre_avis = cours_info.get('Number_of_Reviews', '').replace('avis', '').strip()\n",
    "                duree = cours_info.get('Duration', '')\n",
    "                niveau = cours_info.get('Difficulty', '')\n",
    "                liens = cours_info.get('Courses URLs', '')\n",
    "\n",
    "                # SQL and CSV for sous_sous_formation\n",
    "                sql_sous_sous_formation_insert = f\"INSERT INTO SOUSSOUSDomaineFormation (RefSOUSSOUSDomaineF, RefSOUSDomaineF, Le_nom, Description, Notes, Nombre_avis, Duree, Niveau, Liens, MotsCles) VALUES ('{ref_sous_sous_domaine}', '{ref_sous_domaine}', '{cours_nom_corrigé}', '{description}', '{notes}', '{nombre_avis}', '{duree}', '{niveau}', '{liens}', '{mots_clefs_cours}');\\n\"\n",
    "                sql_sous_sous_formation_file.write(sql_sous_sous_formation_insert)\n",
    "                csv_sous_sous_formation_writer.writerow([ref_sous_sous_domaine, ref_sous_domaine, cours_nom_corrigé, description, notes, nombre_avis, duree, niveau, liens, mots_clefs_cours])\n",
    "\n",
    "                numero_sous_sous_formation += 1\n",
    "\n",
    "            numero_sous_formation += 1\n",
    "\n",
    "        numero_formation += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
